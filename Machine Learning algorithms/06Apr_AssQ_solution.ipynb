{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the mathematical formula for a linear SVM?"
      ],
      "metadata": {
        "id": "a2Tz5qsUOIuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution**\n",
        "\n",
        "The mathematical formula for a linear Support Vector Machine (SVM) can be expressed as follows:\n",
        "\n",
        "Given a set of labeled training data \\((mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\ldots, (\\mathbf{x}_n, y_n)\\), where \\(\\mathbf{x}_i\\) is the \\(i\\)-th input vector and \\(y_i\\) is its corresponding class label (\\(y_i \\in \\{-1, 1\\}\\) for binary classification), the goal of a linear SVM is to find the optimal separating hyperplane defined by:\n",
        "*italicized text*\n",
        "\\[ \\mathbf{w} \\cdot \\mathbf{x} + b = 0 \\]\n",
        "\n",
        "where \\(\\mathbf{w}\\) is the weight vector (normal to the hyperplane) and \\(b\\) is the bias term. The classification decision is made by evaluating the sign of \\(\\mathbf{w} \\cdot \\mathbf{x} + b\\). If it's positive, the point lies on one side of the hyperplane, and if it's negative, the point lies on the other side.\n",
        "\n",
        "The optimization problem for finding the optimal separating hyperplane is typically formulated as:\n",
        "\n",
        "\\[ \\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\]\n",
        "\n",
        "subject to the constraint:\n",
        "\n",
        "\\[ y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1, \\quad \\text{for } i = 1, 2, \\ldots, n \\]\n",
        "\n",
        "This formulation aims to maximize the margin (distance) between the two classes while ensuring that all data points are correctly classified or lie on the correct side of the hyperplane. This problem can be solved using optimization techniques like gradient descent, quadratic programming, or convex optimization methods."
      ],
      "metadata": {
        "id": "e1TTYHtpOMTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the objective function of a linear SVM?"
      ],
      "metadata": {
        "id": "WZTo9__hc6tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective function of a linear Support Vector Machine (SVM) is typically formulated as the minimization of the following expression:\n",
        "\n",
        "min\n",
        "w,b\n",
        "​\n",
        "  \n",
        "2\n",
        "1\n",
        "​\n",
        " ∥w∥\n",
        "2\n",
        "\n",
        "\n",
        "where\n",
        "\n",
        "w is the weight vector and\n",
        "\n",
        "b is the bias term. This objective function aims to find the parameters\n",
        "\n",
        "w and\n",
        "\n",
        "b that define the separating hyperplane with the maximum margin while minimizing the norm of the weight vector.\n",
        "\n",
        "In other words, the SVM seeks to find the hyperplane that maximizes the margin between the classes in the feature space. Maximizing the margin helps improve the generalization performance of the classifier by providing better separation between the classes and reducing the risk of overfitting.\n",
        "\n",
        "The minimization of\n",
        "1\n",
        "2\n",
        "∥\n",
        "�\n",
        "∥\n",
        "2\n",
        "2\n",
        "1\n",
        "​\n",
        " ∥w∥\n",
        "2\n",
        "  is often subject to the constraint:\n",
        "\n",
        "�\n",
        "�\n",
        "(\n",
        "�\n",
        "⋅\n",
        "�\n",
        "�\n",
        "+\n",
        "�\n",
        ")\n",
        "≥\n",
        "1\n",
        "y\n",
        "i\n",
        "​\n",
        " (w⋅x\n",
        "i\n",
        "​\n",
        " +b)≥1\n",
        "\n",
        "This constraint ensures that each training sample is correctly classified or lies on the correct side of the decision boundary with a margin of at least 1."
      ],
      "metadata": {
        "id": "Z5dOfSjxc9bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ikra7HjGdAOt"
      }
    }
  ]
}